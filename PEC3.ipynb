{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PEC 3: Trending Topics\n",
    "\n",
    "## 1. Trending Topics Básico\n",
    "\n",
    "Vamos a aprovechar lo visto sobre Structured Streaming para implementar una versión muy sencilla de \"trending topics\".\n",
    "\n",
    "En el directorio `data/tweets` existen una serie de ficheros CSV. Cada línea de los CSV corresponde a un tweet.\n",
    "\n",
    "Lo que tienes que hacer es un \"job\" de Spark Structured Streaming que vaya procesando los ficheros uno a uno, y mantenga una tabla en memoria con los 20 términos más comentados.\n",
    "\n",
    "Vamos a considerar que los tweets nos llegan en el orden correcto, es decir, no es necesario tener en cuenta el \"timestamp\" de los datos.\n",
    "\n",
    "**Ten en cuenta que hay que resolver este ejercicio usando Structured Streaming**, no se trata de hacer un job Spark estático.\n",
    "\n",
    "El resultado de los cálculos sobre el stream debes almacenarlos en una tabla en memoria `trending`.\n",
    "\n",
    "Por lo tanto, la siguiente query debe ser la que finalmente muestre el resultado (que irá cambiando con el tiempo, según se van procesando nuevos ficheros):\n",
    "\n",
    "```python\n",
    "spark.sql(\"select * from trending\").show()\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql.functions import explode, split\n",
    "#spark = pyspark.sql.SparkSession.builder.getOrCreate()\n",
    "\n",
    "sc = pyspark.SparkContext(appName='app15')\n",
    "spark = pyspark.SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "static = spark.read.csv(\"data/tweets\",sep=\";\",header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- username: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- retweets: string (nullable = true)\n",
      " |-- favorites: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- geo: string (nullable = true)\n",
      " |-- mentions: string (nullable = true)\n",
      " |-- hashtags: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- permalink: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "static.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(username='librarian2277', date='2017-10-14 00:54', retweets='0', favorites='0', text='How to write blog headlines that drive search traffic https:// searchenginewatch.com/2017/10/07/how -to-write-blog-headlines-that-drive-search-traffic/ …', geo=None, mentions=None, hashtags=None, id='919063962876104704', permalink='https://twitter.com/librarian2277/status/919063962876104704')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "static.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lines = spark.readStream.schema(static.schema).option(\"maxFilesPerTrigger\", 1)\\\n",
    "                 .csv(\"data/tweets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words = lines.select(\n",
    "   explode(\n",
    "       split(lines.text, \" \")\n",
    "   ).alias(\"word\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col,lower\n",
    "    \n",
    "words = words.withColumn(\"word\",lower(col(\"word\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#activityCounts = streaming.groupBy(\"mentions\").count()\n",
    "wordCounts = words.groupBy(\"word\").count().orderBy(\"count\",ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "activityQuery = wordCounts.writeStream.queryName(\"trending\")\\\n",
    "                .format(\"memory\")\\\n",
    "                .outputMode(\"complete\").start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<pyspark.sql.streaming.StreamingQuery at 0x794f2680b8>]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.streams.active"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|                word|count|\n",
      "+--------------------+-----+\n",
      "|                    |  152|\n",
      "|                 and|   19|\n",
      "|              search|   15|\n",
      "|                 for|   13|\n",
      "|                 the|   13|\n",
      "|                   i|   12|\n",
      "|                   a|   12|\n",
      "|            https://|   11|\n",
      "|             speech.|   11|\n",
      "|            feminism|   11|\n",
      "|                  to|   11|\n",
      "|                  my|    9|\n",
      "|                poor|    6|\n",
      "|             http://|    6|\n",
      "|                  of|    6|\n",
      "|                this|    6|\n",
      "|                root|    5|\n",
      "|                  it|    5|\n",
      "|               canal|    5|\n",
      "|                 you|    4|\n",
      "|                  is|    4|\n",
      "|                what|    4|\n",
      "|                 all|    4|\n",
      "|                know|    4|\n",
      "|              #hacks|    3|\n",
      "|                 who|    3|\n",
      "|               don't|    3|\n",
      "|do\";;;;\"917692622...|    3|\n",
      "|          #multiples|    3|\n",
      "|                  in|    3|\n",
      "|                they|    3|\n",
      "|               can't|    3|\n",
      "|                   …|    3|\n",
      "|           #children|    3|\n",
      "|                  on|    3|\n",
      "|          #parenting|    3|\n",
      "|                have|    3|\n",
      "|                 etc|    3|\n",
      "|                 are|    3|\n",
      "|                  at|    3|\n",
      "|                   &|    3|\n",
      "|              afford|    3|\n",
      "|               about|    3|\n",
      "|                  or|    3|\n",
      "|             stomach|    2|\n",
      "|                 did|    2|\n",
      "|                 two|    2|\n",
      "|                icon|    2|\n",
      "|       prescriptions|    2|\n",
      "|            research|    2|\n",
      "|               #news|    2|\n",
      "|                 was|    2|\n",
      "|                  #…|    2|\n",
      "|              rescue|    2|\n",
      "|          #rootcanal|    2|\n",
      "|               great|    2|\n",
      "|sucks\";;;;\"399820...|    2|\n",
      "|                toby|    2|\n",
      "|                your|    2|\n",
      "|                free|    2|\n",
      "|            henhouse|    2|\n",
      "|                when|    2|\n",
      "|                 but|    2|\n",
      "|tus/9103684061127...|    2|\n",
      "|                   ||    2|\n",
      "|              public|    2|\n",
      "|                   /|    2|\n",
      "|                want|    2|\n",
      "|                 dan|    2|\n",
      "|                  up|    2|\n",
      "|          extraction|    2|\n",
      "|     #restrelaxation|    2|\n",
      "|                  no|    2|\n",
      "|             cleared|    2|\n",
      "|           wikipedia|    2|\n",
      "|                  an|    2|\n",
      "|twitter.com/carlr...|    2|\n",
      "|            implants|    2|\n",
      "|               would|    2|\n",
      "|              dental|    2|\n",
      "|        https://www.|    2|\n",
      "|     #merrychristmas|    2|\n",
      "|         ache=ulcer:|    2|\n",
      "|                 nw9|    2|\n",
      "|              engine|    2|\n",
      "|             #hacker|    2|\n",
      "|                hope|    1|\n",
      "|     enough......etc|    1|\n",
      "|                 ...|    1|\n",
      "|.\";;;;\"9189837258...|    1|\n",
      "|                some|    1|\n",
      "|…\";;;;\"9189888641...|    1|\n",
      "|              column|    1|\n",
      "|              taking|    1|\n",
      "|me.\";;;;\"53916035...|    1|\n",
      "|                 van|    1|\n",
      "|             traffic|    1|\n",
      "|pfft\";;@sivagiora...|    1|\n",
      "|#fb\";;;#fb;\"48237...|    1|\n",
      "|home.\";;;;\"918941...|    1|\n",
      "+--------------------+-----+\n",
      "only showing top 100 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from trending\").show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "activityQuery.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Eliminando \"stopwords\"\n",
    "Ahora el objetivo es sacar de nuevo los trending topics, pero evitando incluir \"stopwords\". Para ello debes utilizar los datos en `data/stopwords`.\n",
    "\n",
    "Las \"stopwords\" son palabras comunes que no aportan mucho significado.\n",
    "\n",
    "Debes realizar el mismo cálculo de antes sobre el flujo de tweets (de nuevo procesando los ficheros uno a uno), pero esta vez debes evitar que aparezcan stopwords como trending topics. (Ten en cuenta que no deben aparecer stopwords en ninguna combinación de mayúsculas/minúsculas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "txt = open(\"data/stopwords/stopwords.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stopwords = []\n",
    "for fila in txt:\n",
    "    stopwords.append(fila.lower().strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words2 = lines.select(\n",
    "    explode(\n",
    "       split(lines.text, \" \")\n",
    "   ).alias(\"word\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words2 = words.withColumn(\"word\",lower(col(\"word\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wordCounts2 = words2.groupBy(\"word\").count().orderBy(\"count\",ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "wordCounts3 = wordCounts2.filter(col(\"word\").isin(stopwords)==False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "activityQuery2 = wordCounts3.writeStream.queryName(\"trending2\").format(\"memory\").outputMode(\"complete\").start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|                word|count|\n",
      "+--------------------+-----+\n",
      "|                    |  152|\n",
      "|              search|   15|\n",
      "|            https://|   11|\n",
      "|             speech.|   11|\n",
      "|            feminism|   11|\n",
      "|                poor|    6|\n",
      "|             http://|    6|\n",
      "|                root|    5|\n",
      "|               canal|    5|\n",
      "|              #hacks|    3|\n",
      "|               don't|    3|\n",
      "|do\";;;;\"917692622...|    3|\n",
      "|          #multiples|    3|\n",
      "|               can't|    3|\n",
      "|                   …|    3|\n",
      "|           #children|    3|\n",
      "|          #parenting|    3|\n",
      "|                   &|    3|\n",
      "|              afford|    3|\n",
      "|             stomach|    2|\n",
      "+--------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from trending2  limit 20\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "activityQuery2.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Evitar signos de puntuación\n",
    "Ya hemos eliminado las stopwords, ahora sólo nos queda evitar que en nuestros trending topics aparezcan palabras que consisten sólo en signos de puntuación.\n",
    "¿Cómo lo harías?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from pyspark.sql.functions import regexp_replace\n",
    "from pyspark.sql.functions import col, lower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words = lines.select(\n",
    "    explode(\n",
    "       split(lines.text, \" \")\n",
    "   ).alias(\"word\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = words.withColumn(\"word\",lower(col(\"word\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words3 = words.withColumn(\"word\",regexp_replace(col(\"word\"), '[^a-z]', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wordCounts3 = words3.groupBy(\"word\").count().orderBy(\"count\",ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordCounts3 = wordCounts3.filter(col(\"word\").isin(stopwords)==False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "activityQuery3 = wordCounts3.writeStream.queryName(\"trending7\").format(\"memory\").outputMode(\"complete\").start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|                word|count|\n",
      "+--------------------+-----+\n",
      "|                    |  184|\n",
      "|              search|   16|\n",
      "|                 cnn|   11|\n",
      "|bbchttpstwitterco...|   11|\n",
      "|            feminism|   11|\n",
      "|              speech|   11|\n",
      "|               https|   11|\n",
      "|                poor|    7|\n",
      "|                http|    6|\n",
      "|                root|    5|\n",
      "|               canal|    5|\n",
      "|dohttpstwittercom...|    3|\n",
      "|           multiples|    3|\n",
      "|           parenting|    3|\n",
      "|            children|    3|\n",
      "|            implants|    3|\n",
      "|              dental|    3|\n",
      "|              afford|    3|\n",
      "|                dont|    3|\n",
      "|               hacks|    3|\n",
      "+--------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from trending7 limit 20\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "activityQuery3.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
